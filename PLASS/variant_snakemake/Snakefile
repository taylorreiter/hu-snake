ENV = "env.yml"
ENV2 = "env2.yml"
NBHD = "hu-genome41"
TRIM = ["hard"]
PFAM_BASE=["PF00521_gyra"]

rule all:
    input: 
        #"outputs/paladin/{nbhd}.hardtrim.sam.flagstat"
        #expand("outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-dom.out", nbhd = NBHD, trim = TRIM, pfam_base = PFAM_BASE),
        #expand("outputs/paladin/{nbhd}.{trim}trim.sort.bam.bai", nbhd = NBHD, trim = TRIM, pfam_base = PFAM_BASE)
        #expand("outputs/comp/{nbhd}_{trim}trim_{pfam_base}_k31.csv", nbhd = NBHD, trim = TRIM, pfam_base = PFAM_BASE) 
        #dynamic("outputs/megahit/cut{i}/cut{i}.contigs.fa")
        #dynamic("outputs/megahit/cut{i}-longest.fa")
        #dynamic("outputs/faa/cut{i}.faa"),
        #dynamic("outputs/ffn/cut{i}.ffn")
        #dynamic("outputs/transdecoder/cut{i}_longest_orfs.pep")
        dynamic("outputs/hmmscan_orf/cut{i}-dom.out")

# run per nbhd (definitely) 
# & per pfam domain? 

rule download_plass:
    output: "inputs/plass/{nbhd}.{trim}trim.plass.c100.fa" # shouldn't need to use clean if only dealing with one nbhd
    shell:'''
    touch {output} #place holder for downloading plass hardtrim results
    '''

rule format_plass_cut:
    output: "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.fa"
    input: "inputs/plass/{nbhd}.{trim}trim.plass.c100.fa"
    shell:'''
    cut -d ' ' -f1 {input} > {output}
    '''

rule format_plass_dup:
    output: "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa"
    input: "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.fa"
    shell:'''
    awk '(/^>/ && s[$0]++){{$0=$0"_"s[$0]}}1;' {input} > {output}
    '''

rule download_pfam:
    output: "inputs/pfam/{pfam_base}.sto"
    params: 
        out_dir = "inputs/pfam"
    shell:'''
    pfam=$(echo {wildcards.pfam_base} | cut -f1 -d"_")
    wget -O {output} https://pfam.xfam.org/family/${{pfam}}/alignment/full
    '''

rule hmmbuild:
    input: "inputs/pfam/{pfam_base}.sto"
    output: "outputs/hmmbuild/{pfam_base}.hmm"
    shell: '''
    hmmbuild outputs/hmmbuild/{wildcards.pfam_base}.hmm inputs/pfam/{wildcards.pfam_base}.sto   
    hmmpress outputs/hmmbuild/{wildcards.pfam_base}.hmm
'''

rule hmmscan:
    output:
        out = "outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100.out",
        tbl = "outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-tbl.out",
        dom = "outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-dom.out"
    input:
        hmm = "outputs/hmmbuild/{pfam_base}.hmm",
        faa = "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa"
    conda: ENV
    shell:'''
    hmmscan -T 100 -o {output.out} --tblout {output.tbl} --domtblout {output.dom} {input.hmm} {input.faa} 
'''

rule get_rhmmer:
    output: "rhmmer.R"
    shell:'''
    wget -O rhmmer.R https://raw.githubusercontent.com/arendsee/rhmmer/master/R/parse.R
    '''

rule def_overlap_window_and_find_reads:
# get the names of the PLASS assembled aa seqs that match the PFAM
# domain of interest.
    input:
        dom = "outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-dom.out",
        rhmmer = "rhmmer.R"
    output:
        keep = "outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-NAMES.txt"
    conda: ENV
    script:'variant-workflow-hmm-matches.R'

rule paladin_index:
    output: "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa.bwt"
    input: "outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa"
    conda: ENV
    shell:'''
    paladin index -r3 {input}
    '''

rule paladin_align:
    output: "outputs/paladin/{nbhd}.{trim}trim.sam"
    input:
        indx="outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa.bwt",
        reads="inputs/reads/{nbhd}.{trim}trim.reads.gz"
    params: 
        indx="outputs/plass/{nbhd}.{trim}trim.plass.c100.cut.dup.fa"
    conda: ENV
    shell:'''
    paladin align -f 125 -t 2 {params.indx} {input.reads} > {output}
    '''

rule samtools_flagstat_paladin:
    output: "outputs/paladin/{nbhd}.{trim}trim.sam.flagstat"
    input: "outputs/paladin/{nbhd}.{trim}trim.sam"
    conda: ENV
    shell:'''
    samtools flagstat {input} > {output}
    '''

rule samtools_view_paladin:
    output: "outputs/paladin/{nbhd}.{trim}trim.bam"
    input: "outputs/paladin/{nbhd}.{trim}trim.sam"
    conda: ENV
    shell:'''
    samtools view -b {input} > {output}
    '''

rule samtools_sort_paladin:
    output: "outputs/paladin/{nbhd}.{trim}trim.sort.bam"
    input: "outputs/paladin/{nbhd}.{trim}trim.bam"
    conda: ENV
    shell:'''
    samtools sort {input} > {output}
    '''

rule samtools_index_paladin:
    output: "outputs/paladin/{nbhd}.{trim}trim.sort.bam.bai"
    input: "outputs/paladin/{nbhd}.{trim}trim.sort.bam"
    conda: ENV
    shell:'''
    samtools index {input} 
    '''

rule extract_plass_readnames_from_bam:
# extract names of reads for each aa seq bam section that matched 
# PFAM domain of interest
    output: dynamic("outputs/bam_subsets/{plass_match}-NAMES.txt") 
    input:
        plass_names = expand("outputs/hmmscan/{nbhd}.{trim}trim.plass.{pfam_base}-hmmscanT100-NAMES.txt", nbhd = NBHD, trim = TRIM, pfam_base=PFAM_BASE), 
        bai = expand("outputs/paladin/{nbhd}.{trim}trim.sort.bam.bai", nbhd = NBHD, trim = TRIM), 
        bam = expand("outputs/paladin/{nbhd}.{trim}trim.sort.bam", nbhd = NBHD, trim = TRIM) 
    run:
        import pysam
        import re
        
        with open(str(input.plass_names)) as f:
            plass_hmmscan_matches = f.readlines()

        plass_hmmscan_matches = [x.strip() for x in plass_hmmscan_matches] 

        samfile = pysam.AlignmentFile(str(input.bam), 'rb')

        for match in plass_hmmscan_matches:
            reads = []
            for read in samfile.fetch(match):
                name = re.sub('.*:', '', read.qname)
                reads.append(name)
                with open(f"outputs/bam_subsets/{match}-NAMES.txt", 'w') as outfile:
                    for s in reads:
                        outfile.write("%s\n" % s)
        samfile.close()


rule grab_plass_reads:
    output: "outputs/bam_subsets/{plass_match}.fa" # loses fq info
    input: 
        names = "outputs/bam_subsets/{plass_match}-NAMES.txt",
        reads=expand("inputs/reads/{nbhd}.{trim}trim.reads.gz", nbhd = NBHD, trim = TRIM)
    conda: ENV
    shell:'''
    ./extract-hmmscan-matches.py {input.names} {input.reads}  > {output} 
    '''

rule calc_plass_read_sigs:
    output: "outputs/bam_subsets/{plass_match}.sig"
    input: "outputs/bam_subsets/{plass_match}.fa"
    conda: ENV
    shell:'''
    sourmash compute -k 31 --scaled 1 -o {output} {input}
    '''

rule compare_plass_read_sigs:
    output: "outputs/comp/{nbhd}_{trim}trim_{pfam_base}_k31.csv"
    input: dynamic("outputs/bam_subsets/{plass_match}.sig")
    conda: ENV
    shell:'''
    sourmash compare -k 31 --csv {output} {input}
    '''

rule cut_dendo:
    output: dynamic("outputs/comp/cut{i}.txt")
    input: comp = expand("outputs/comp/{nbhd}_{trim}trim_{pfam_base}_k31.csv", nbhd = NBHD, trim = TRIM, pfam_base = PFAM_BASE)
    conda: ENV
    script: 'variant-workflow-cut-dendo.R'

rule cat_dendo_reads:
    output: "outputs/comp/cut{i}.fa"
    input: "outputs/comp/cut{i}.txt"
    run:
        import re
        print(str(input))	
        with open(str(input)) as f:
            cut_bams = f.readlines()
            cut_bams = [x.strip() for x in cut_bams] 
            cut_bam_reads = ["outputs/bam_subsets/" + s + ".fa" for s in cut_bams]
            cut_base = re.sub('.*/', '', str(input))
            cut_base = re.sub(".txt", '', cut_base)
            with open('outputs/comp/' + cut_base + '.fa', 'w') as outfile:
                for bam_reads in cut_bam_reads:
                    with open(bam_reads) as infile:
                        for line in infile:
                            outfile.write(line) 

rule extract_dendo_paired_reads:
    output:
        pairs = "outputs/dendo/cut{i}_paired.fa",
        single = "outputs/dendo/cut{i}_single.fa"
    input: "outputs/comp/cut{i}.fa"
    conda: ENV
    shell:'''
    extract-paired-reads.py -p {output.pairs} -s {output.single} {input}
    '''

rule megahit_dendo_paired_reads:
    output: "outputs/megahit/cut{i}/cut{i}.contigs.fa"
    input: "outputs/dendo/cut{i}_paired.fa"
    conda: ENV
    shell:'''
    megahit --12 {input} -o outputs/megahit/cut{wildcards.i} --out-prefix cut{wildcards.i} --continue
    '''

#rule faidx_megahit:
#    output: "outputs/megahit/cut{i}/cut{i}.contigs.fa.fai"
#    input: "outputs/megahit/cut{i}/cut{i}.contigs.fa"
#    conda: ENV
#    shell:'''
#    samtools faidx {input}
#    '''

#rule select_longest_megahit_contig:
#    output: "outputs/megahit/cut{i}-longest.fa"
#    input: 
#        fai="outputs/megahit/cut{i}/cut{i}.contigs.fa.fai",
#        fa="outputs/megahit/cut{i}/cut{i}.contigs.fa"
#    conda: ENV
#    shell:'''
#    var=`sort -k2,2nr -k2,2 {input.fai} | head -n 1 | cut -f1`
#    samtools faidx {input.fa} $var > {output}
#    '''

#rule prokka:
#    output: 
#        gff="outputs/prokka/cut{i}.gff",
#        faa="outputs/prokka/cut{i}.faa",
#        fna="outputs/prokka/cut{i}.ffn"
#    input: "outputs/megahit/cut{i}/cut{i}.contigs.fa"
#    conda: ENV
#    params:
#        outdir = 'outputs/prokka',
#        nbhd = NBHD
#    shell:'''
#    prokka {input} --outdir {params.outdir} --prefix cut{wildcards.i} --metagenome --force --locustag {params.nbhd}-cut{wildcards.i}
    # touch {output.faa}
#    '''

rule transdecoder:
    output: 
        cds="outputs/transdecoder/cut{i}_longest_orfs.cds",
        gff="outputs/transdecoder/cut{i}_longest_orfs.gff3",
        pep="outputs/transdecoder/cut{i}_longest_orfs.pep"
    input: "outputs/megahit/cut{i}/cut{i}.contigs.fa"
    params: outdir = "outputs/transdecoder"
    conda: ENV
    shell:'''
    TransDecoder.LongOrfs -t {input} 
    rm -rf *checkpoints_longorfs/
    mv cut{wildcards.i}.contigs.fa.transdecoder_dir/longest_orfs.cds {params.outdir}/cut{wildcards.i}_longest_orfs.cds
    mv cut{wildcards.i}.contigs.fa.transdecoder_dir/longest_orfs.gff3 {params.outdir}/cut{wildcards.i}_longest_orfs.gff3
    mv cut{wildcards.i}.contigs.fa.transdecoder_dir/longest_orfs.pep {params.outdir}/cut{wildcards.i}_longest_orfs.pep
    rm -rf cut{wildcards.i}.contigs.fa.transdecoder_dir
    '''

rule hmmscan_orfs:
    output:
        dom = "outputs/hmmscan_orf/cut{i}-dom.out"
    input:
        hmm = expand("outputs/hmmbuild/{pfam_base}.hmm", pfam_base = PFAM_BASE),
        pep = "outputs/transdecoder/cut{i}_longest_orfs.pep"
    conda: ENV
    shell:'''
    hmmscan -T 100 --domtblout {output.dom} {input.hmm} {input.pep} 
    '''

rule find_gyra_seqs:
    output:
        loci="outputs/loci/cut{i}-NAMES.txt"
    input: 
        gff="outputs/prokka/cut{i}.gff"
    conda: ENV2
    script:"variant-workflow-grab-loci.R"

rule grab_gyra_nucs:
    output: "outputs/ffn/cut{i}.ffn"
    input:
        loci = "outputs/loci/cut{i}-NAMES.txt",
        ffn = "outputs/prokka/cut{i}.ffn"
    conda: ENV
    shell:'''
    ./extract-hmmscan-matches.py {input.loci} {input.ffn} > {output} 
    '''

rule grab_gyra_aas:
    output:"outputs/faa/cut{i}.faa"
    input:
        loci = "outputs/loci/cut{i}-NAMES.txt",
        faa = "outputs/prokka/cut{i}.faa"
    conda: ENV
    shell:'''
    ./extract-hmmscan-matches.py {input.loci} {input.faa} > {output}
    '''
